### **研究领域现状**

1. **主流方法**：
   - **基于序列建模**：如GRU4Rec、NARM，依赖当前会话的时序模式，但忽略用户历史行为。
   - **基于图神经网络（GNN）**：如SR-GNN、GCE-GNN，将会话转为图结构，但仅聚焦当前会话的局部物品转移。
   - **个性化方法**：如H-RNN、A-PGNN，利用用户历史会话，但未跨用户挖掘全局模式。
2. **局限性**：
   - **非个性化推荐**：匿名用户假设下，无法建模用户长期偏好。
   - **信息孤岛**：现有方法仅利用单用户历史，忽视跨用户的全局物品关联（如共现、转移）。

------

### **研究领域痛点**

1. **个性化缺失**：
   - 匿名会话场景下，用户历史行为未被有效利用。
   - 即使已知用户身份，现有模型未充分结合长期偏好与当前兴趣。
2. **全局信息利用不足**：
   - 物品的跨会话关联（如共现、相似性）未被建模。
   - 例如，不同用户的相似会话可能隐含潜在物品关联，但现有方法无法捕捉。

------

### **核心方法（HG-GNN）**

1. **异构全局图构建**：
   - **节点类型**：用户节点（U）、物品节点（V）。
   - **边类型**：
     - **用户-物品交互边**（U→V）：捕获长期偏好。
     - **会话内转移边**（V→V，方向敏感）：相邻物品转移。
     - **全局共现边**（V↔V）：跨会话高频共现物品关联。
   - **采样策略**：保留Top-K高频边以控制计算复杂度。
2. **异构图神经网络（HGNN）**：
   - **分层聚合**：针对不同边类型设计独立参数，分层聚合邻居信息。
     - 公式示例：
       pvi(k+1)=mean(pvi,rin(k+1),pvi,rout(k+1),pvi,rsimilar(k+1))**p***v**i*​(*k*+1)​=mean(**p***v**i*​,*r*in​(*k*+1)​,**p***v**i*​,*r*out​(*k*+1)​,**p***v**i*​,*r*similar​(*k*+1)​)
   - **多层级表示融合**：将各层GNN输出加权求和，增强语义表达（式8）。
3. **个性化会话编码器**：
   - **当前偏好学习**：基于注意力机制捕捉会话内关键物品（式11-13）。
   - **长期偏好学习**：通过用户嵌入（HGNN输出）与当前会话物品的注意力融合（式14-16）。
   - **动态门控融合**：结合当前偏好（Cu**C***u*）与长期偏好（Ou**O***u*），生成最终会话表示（式17-18）。

------

### **创新点**

1. **全局异构图结构**：
   - 首次在会话推荐中整合用户行为、跨会话物品转移与共现关系，打破数据孤岛。
2. **动态偏好建模**：
   - 通过门控机制平衡用户长期兴趣与当前会话动态，提升个性化能力。
3. **高效计算设计**：
   - 边采样（Top-K）与分层参数共享，降低计算开销，适合大规模场景。

------

### **评估指标与实验**

1. **指标**：
   - **HR@k**（命中率）：目标物品是否出现在前k个推荐中。
   - **MRR@k**（平均倒数排名）：目标物品排名的倒数均值（衡量排名质量）。
2. **实验结果**：
   - **SOTA对比**：
     - **Last.fm**：HR@5=13.09%（vs A-PGNN 12.10%），MRR@5=7.35%。
     - **Reddit**：HR@5=51.08%（提升4.03%），MRR@5=35.46%（提升5.72%）。
   - **消融实验**：
     - 移除用户节点：HR@5下降约1.5%（Last.fm），证明用户信息的重要性。
     - 移除共现边：HR@5下降0.8%，说明全局关联的必要性。
3. **效率**：
   - 训练速度优于A-PGNN（无Transformer模块）和GCE-GNN（无多图计算）。
